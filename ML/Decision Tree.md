# Decision Tree

## 简介

决策树数是常见的分类学习任务

* **基本流程**

<img src="https://img-blog.csdnimg.cn/20181223220721212.jpg" alt="img" style="zoom: 80%;" />



## 基本原理

* **信息论基础**

**信息熵**：

**联合熵**：

**条件熵**：



分类决策树=结点+有向边

只有一层划分的决策树亦称决策桩

- ### 结点

  - 内部结点（特征或属性）**根结点+子结点**
  - 叶结点（类别）

- ### 过程简述 

  **根结点**开始，测试某一**特征**，根据测试结果分配到**子结点**

  每一个子结点对应该**特征**的一个取值

  最后达到**叶结点**，分配到叶结点的**类**中

  > 特征 ：脐部，色泽，根蒂等
  >
  > 类：好瓜，坏瓜

- ### 具体过程（结合西瓜书上的例子看

  1）**输入确定训练集D，属性集A，结点node**

> 属性即是某一具体特征，如“色泽”、“根蒂”等，每个内部结点都对应一个属性

​	2）**如果D中样本都属于同一个类别C，则node为叶结点（c类）。**

> 叶结点一般对应某一决策结果，如“是好瓜”、“不是好瓜”

​	3）**如果A为空0或D在样本A上取值相同<sub>（1）</sub>，**

​	**将node标记为叶结点，类别为D中样本数最多的类。$_{(2)}$**

> 举一个A为空的例子，当敲声为浊响的时候，**不存在继续判断的属性**（所有属性先前都已经判断过了），已经可以直接判断类了。
>
> (1).D中所有元素在A中所有判定特征下，取值都相同，即无论用什么条件判断都得到同一个答案 。
>
> (2).样本即为输入训练集中的元素，叶结点标记为同一属性的各个类之中含有样本数最多的类

​	4）**从A中选择最优划分属性$a_*$ ，对于$a_*$中的每一个值$a^v_*$,获得D在$a_*$中取$a^v_*$的子集D<sub>v</sub>。**

> 最优划分属性指与决策结果相关性最大的属性
>
> a<sub>*</sub>中的值指的是对于同一属性中的不同类

​	5）**如果D<sub>v</sub>为空则分支为叶结点，如果非空则为内部结点（继续循环）。**

> 假设所有瓜的色泽均是青绿，则色泽非青绿的类别结点则是叶结点（不继续延伸）
>
> 非空时，把类别设定为父结点所含样本最多的类别，？
>
> 3）为当前结点的后验分布，5）把父结点的样本分布当做先验分布

- ### 划分选择

  决策树的关键步骤为选择**最优划分属性**

  决策树的分支结点所包含的样本尽可能属于同一类别，即结点“纯度”越来越高。

  > 通过判断使某结点样本间的差异不断减小

- ### 信息增益

  - #### 信息熵

    信息熵指度量样本集合**纯度**的指标

    假定D中第k**类**样本所占**比例**为$p_k$,则信息熵$Ent(D) = -\displaystyle \sum_{k=1}^{|y|}{p_klog_2p_k}$。（k=1,2,...,|y|)

    > Ent(D)越小，纯度越高。（虽然加了负号，但$p_k$小于2，所以结果为正）
    >
    > 判断好瓜坏瓜时，y=2。
    >
    > 比如说
    >
    > 计算色泽的信息熵，八个好瓜类，九个坏瓜类pk=8/17。
    >
    > 计算色泽=青绿的信息熵，以数据集2.0为例，样本青绿D1有三个好瓜类，三个坏瓜类，故pk=3/6。
    >
    > pk=1时纯度最高，即所有样本属于同一类。

  - #### 信息增益

    假设a有V个可能取值，则会产生V个结点，分别包含元素$D^v$,对其赋予**权重$\frac{|D^v|}{|D|}$**。于是可以算出属性a对样本集D划分所得的“信息增益”

    $Gain(D,a) = Ent(D) - \displaystyle \sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)$. **此式中D和$D^v$均指样本数**

    > **信息增益越大**，使用属性a划分获得的纯度提升越**大**
    >
    > 所以$a_*= argmax Gain(D,a)$.
    >
    > ID3决策树学习算法就是以信息增益为准则划分属性
    >
    > 比如计算色泽的信息增益
    >
    > 色泽=青绿占样本的6/17，色泽=乌黑占6/17，色泽=浅白占7/17.
    >
    > 用他们的权重比分别乘以其信息熵，求和取负即得到信息增益。

  - #### 增益率

    信息增益准则对**可取值数目较多的属性**有所偏好，为减少这种不利影响**C4.5决策树算法**使用“增益率”来确定最优划分属性。

    增益率定义为$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$.

    其中,$IV(a)=- \displaystyle \sum^{V}_{v=1}\frac{|D^V|}{|D|}log_2\frac{|D^V|}{|D|}$.称为属性a的“固有值”，属性a的V越大，IV（a）的值通常越大。（注意与信息熵的区别）

    **增益率准则对可取值数目较少的属性有所偏好**

    因此C4.5决策树算法使用一个启发式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。**

  - #### 基尼指数

    CART决策树使用基尼指数选择划分属性，基尼值：

    $Gini(D)=\displaystyle\sum^{|y|}_{k=1}\displaystyle\sum_{k'\neq k}p_kp_k'$=$1-\displaystyle\sum^{|y|}_{k=1}p_k^2$.

    基尼值表示纯度,反映了从数据集中随机抽取两个样本，其**类别标记不一致**的概率，**值越小，纯度越高**。

    $Gini\_index(D,a)=\displaystyle\sum^{V}_{v=1}\frac{|D^V|}{|D|}Gini(D^v)$.

    于是在A中选择$a_*$时，选择使划分后基尼指数最小的属性作为最优划分属性

- ### 剪枝处理

  剪枝是算法中对付**过拟合**的主要手段，即主动去掉一些分支。

  > - 关于过拟合：
  >   - 当学习器把训练样本学得“太好了”（分类精度为100%，或接近）的时候，可能把**训练样本**的一些特点当做了所有**潜在样本**都会具有的一般性质，这样会导致泛化性能下降。
  >   - 导致因素：学习能力过于强大（？？？！
  >   - 解决方法：（无法彻底避免）
  > - （补充）欠拟合：
  >
  > 训练样本对一般性质尚未学好
  >
  > - 关于泛化误差：
  >
  > 学习器在训练集上的误差称为“训练误差”或“经验误差”，在新样本上的误差称为”泛化误差“

- #### 基本策略

- ##### 预剪枝

  在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并标记为叶结点。

  > - 步骤
  >
  > 训练集判断得到的分类，去验证测试集的正确率
  >
  > 在判断叶结点是否需要划分时，对于父结点已经划分的属性，不满足条件的，按照父结点的划分结果算
  >
  > 例如
  >
  > 在脐部=凹陷判断是否划分色泽时，对于训练集中脐部非凹陷的样本，判定结果按脐部划分时的判定结果算（即划分脐部时判断该样本是否划分正确）
  >
  > - 优缺点
  >
  > 预剪枝使得很多分支没有展开，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间。但是，有些分支虽当前不能提升泛化性。甚至可能导致泛化性暂时降低，但在其基础上进行后续划分却有可能导致显著提高，因此预剪枝的这种贪心本质，给决策树带来了欠拟合的风险。

- ##### 后剪枝

  先从训练集生成一颗完整的决策树然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将子树替换为叶结点。

  > - 步骤
  >
  > 每个结点（无论是叶结点还是子结点）开始，计算若不划分该结点的父结点的精度，并和决策树的精度比较。
  >
  > - 优缺点
  >
  > 后剪枝过程是从低往上剪裁，其训练时间开销比较大。但后剪枝可以比预剪枝保留更多的分支，欠拟合风险小。

- ##### 泛化性能

  > 如何判断泛化性能提升：

> 评估方法这里使用**留出法**。
>
> **留出法**即将数据及划分为两个互斥的集合，一个为训练集S，另一个为测试集（验证集）T。
>
> - 要点
>
> 避免数据划分引入的额外误差
>
> 若干次随机划分、重复试验后取平均值
>
> - 问题
>
> 若S中包含大多数样本，则训练出的模型可能更接近用D训练的模型，但由于T较小，评估结果课能不够稳定。若T多包含一些样本，则S和D差别更大了，降低了评估结果的保真性。所以通常将D中$\frac{2}{3}$到$\frac{4}{5}$的样本作为S。

- ### 连续与缺失值

  - ##### 连续值

    连续属性的可取值不再有限，不能根据结点划分，要使用连续属性离散化技术，比如二分法。（C4.5决策树算法）

    > - 连续属性
    >
    > 如密度、含糖率等没有明显界限的属性。
    >
    > 连续属性仍然可以作为子结点的划分属性。
    >
    > - 二分法
    >
    > 假定a在D上出现了n个取值，将其从小到大排序后，基于划分点t将D分为两部分。
    >
    > 每次选取相邻两个数的平均数作为t，然后像离散值一样考察这些划分点。
    >
    > 计算信息增益$Gain(D,a,t)$，并选择使其最大的划分点。

  - ##### 缺失值

    部分样本的某些属性值可能未知（比如未知色泽等等）

    - 问题

      1.属性值缺失时怎么划分属性

      2.给定划分属性，属性缺失样本怎么划分

    - 解决方法

      